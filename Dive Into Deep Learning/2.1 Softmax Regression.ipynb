{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd30efdc",
   "metadata": {},
   "source": [
    "# 2.1 Softmax Regression\n",
    "\n",
    "Colloquially, machine learning practitioners\n",
    "overload the word *classification*\n",
    "to describe two subtly different problems:\n",
    "(i) those where we are interested only in\n",
    "hard assignments of examples to categories (classes);\n",
    "and (ii) those where we wish to make soft assignments,\n",
    "i.e., to assess the probability that each category applies.\n",
    "The distinction tends to get blurred, in part,\n",
    "because often, even when we only care about hard assignments,\n",
    "we still use models that make soft assignments.\n",
    "\n",
    "In general, classification problems do not come\n",
    "with natural orderings among the classes.\n",
    "Fortunately, statisticians invented a simple way\n",
    "to represent categorical data: the *one-hot encoding*.\n",
    "A one-hot encoding is a vector\n",
    "with as many components as we have categories.\n",
    "The component corresponding to a particular instance's category is set to 1\n",
    "and all other components are set to 0.\n",
    "\n",
    "\n",
    "## 2.1.1 Classification\n",
    "\n",
    "### Linear Model\n",
    "In order to estimate the conditional probabilities\n",
    "associated with all the possible classes,\n",
    "we need a model with multiple outputs, one per class.\n",
    "\n",
    "In our case, since we have 4 features and 3 possible output categories,\n",
    "we need 12 scalars to represent the weights ($w$ with subscripts),\n",
    "and 3 scalars to represent the biases ($b$ with subscripts). This yields:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "o_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\\\\n",
    "o_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\\\\n",
    "o_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For a more concise notation we use vectors and matrices:\n",
    "$\\mathbf{o} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}$\n",
    "\n",
    "### The Softmax\n",
    "\n",
    "Treating classification as a vector-valued regression problem that directly minimizes the difference\n",
    "between $\\mathbf{o}$ and the labels $\\mathbf{y}$ has two drawbacks:\n",
    "* There is no guarantee that the outputs $o_i$ sum up to $1$ in the way we expect probabilities to behave.\n",
    "* There is no guarantee that the outputs $o_i$ are even nonnegative, even if their outputs sum up to $1$, or that they do not exceed $1$.\n",
    "\n",
    "One way to accomplish this goal is to use\n",
    "an exponential function $P(y = i) \\propto \\exp o_i$.This does indeed satisfy the requirement\n",
    "that the conditional class probability\n",
    "increases with increasing $o_i$, it is monotonic,\n",
    "and all probabilities are nonnegative.\n",
    "We can then transform these values so that they add up to $1$\n",
    "by dividing each by their sum.\n",
    "This process is called *normalization*.\n",
    "Putting these two pieces together\n",
    "gives us the *softmax* function:\n",
    "\n",
    "$$\\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{o}) \\quad \\textrm{where}\\quad \\hat{y}_i = \\frac{\\exp(o_i)}{\\sum_j \\exp(o_j)}.$$\n",
    "\n",
    "Note that the largest coordinate of $\\mathbf{o}$\n",
    "corresponds to the most likely class according to $\\hat{\\mathbf{y}}$.\n",
    "Moreover, because the softmax operation\n",
    "preserves the ordering among its arguments,\n",
    "we do not need to compute the softmax\n",
    "to determine which class has been assigned the highest probability. Thus,\n",
    "\n",
    "$$\n",
    "\\operatorname*{argmax}_j \\hat y_j = \\operatorname*{argmax}_j o_j.\n",
    "$$\n",
    "\n",
    "### Vectorization\n",
    "Assume that we are given a minibatch $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$\n",
    "of $n$ examples with dimensionality (number of inputs) $d$.\n",
    "Moreover, assume that we have $q$ categories in the output.\n",
    "Then the weights satisfy $\\mathbf{W} \\in \\mathbb{R}^{d \\times q}$\n",
    "and the bias satisfies $\\mathbf{b} \\in \\mathbb{R}^{1\\times q}$.\n",
    "\n",
    "$$ \\begin{aligned} \\mathbf{O} &= \\mathbf{X} \\mathbf{W} + \\mathbf{b}, \\\\ \\hat{\\mathbf{Y}} & = \\mathrm{softmax}(\\mathbf{O}). \\end{aligned} $$\n",
    "\n",
    "This accelerates the dominant operation into\n",
    "a matrix--matrix product $\\mathbf{X} \\mathbf{W}$.\n",
    "Moreover, since each row in $\\mathbf{X}$ represents a data example,\n",
    "the softmax operation itself can be computed *rowwise*:\n",
    "for each row of $\\mathbf{O}$, exponentiate all entries\n",
    "and then normalize them by the sum. Note, though, that care must be taken\n",
    "to avoid exponentiating and taking logarithms of large numbers,\n",
    "since this can cause numerical overflow or underflow.\n",
    "Deep learning frameworks take care of this automatically.\n",
    "\n",
    "## 2.1.2 Loss Functoin\n",
    "\n",
    "### Log-Likelihood\n",
    "The softmax function gives us a vector $\\hat{\\mathbf{y}}$,\n",
    "which we can interpret as the (estimated) conditional probabilities\n",
    "of each class, given any input $\\mathbf{x}$. In the following we assume that for a dataset\n",
    "with features $\\mathbf{X}$ the labels $\\mathbf{Y}$\n",
    "are represented using a one-hot encoding label vector.\n",
    "\n",
    "$$\n",
    "P(\\mathbf{Y} \\mid \\mathbf{X}) = \\prod_{i=1}^n P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)}).\n",
    "$$\n",
    "\n",
    "We are allowed to use the factorization\n",
    "since we assume that each label is drawn independently\n",
    "from its respective distribution $P(\\mathbf{y}\\mid\\mathbf{x}^{(i)})$.\n",
    "\n",
    "We take the negative logarithm to obtain the equivalent problem\n",
    "of minimizing the negative log-likelihood:\n",
    "\n",
    "$$\n",
    "-\\log P(\\mathbf{Y} \\mid \\mathbf{X}) = \\sum_{i=1}^n -\\log P(\\mathbf{y}^{(i)} \\mid \\mathbf{x}^{(i)})\n",
    "= \\sum_{i=1}^n l(\\mathbf{y}^{(i)}, \\hat{\\mathbf{y}}^{(i)}),\n",
    "$$\n",
    "\n",
    "where for any pair of label $\\mathbf{y}$\n",
    "and model prediction $\\hat{\\mathbf{y}}$\n",
    "over $q$ classes, the loss function $l$ is\n",
    "\n",
    "$$ l(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{j=1}^q y_j \\log \\hat{y}_j. $$\n",
    "\n",
    "The loss function is commonly called the *cross-entropy loss*.\n",
    "\n",
    "### Softmax and Cross-Entropy Loss\n",
    "\n",
    "Plugging $\\hat{\\mathbf{y}} = \\mathrm{softmax}(\\mathbf{o}) \\quad \\textrm{where}\\quad \\hat{y}_i = \\frac{\\exp(o_i)}{\\sum_j \\exp(o_j)}.$ into the definition of the loss and using the definition of the softmax we obtain\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "l(\\mathbf{y}, \\hat{\\mathbf{y}}) &=  - \\sum_{j=1}^q y_j \\log \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} \\\\\n",
    "&= \\sum_{j=1}^q y_j \\log \\sum_{k=1}^q \\exp(o_k) - \\sum_{j=1}^q y_j o_j \\\\\n",
    "&= \\log \\sum_{k=1}^q \\exp(o_k) - \\sum_{j=1}^q y_j o_j.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "consider the derivative with respect to any logit $o_j$. We get\n",
    "\n",
    "$$\n",
    "\\partial_{o_j} l(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} - y_j = \\mathrm{softmax}(\\mathbf{o})_j - y_j.\n",
    "$$\n",
    "\n",
    "In other words, the derivative is the difference\n",
    "between the probability assigned by our model,\n",
    "as expressed by the softmax operation,\n",
    "and what actually happened, as expressed\n",
    "by elements in the one-hot label vector.In this sense, it is very similar\n",
    "to what we saw in regression,\n",
    "where the gradient was the difference\n",
    "between the observation $y$ and estimate $\\hat{y}$.\n",
    "This is not a coincidence.\n",
    "In any exponential family model,\n",
    "the gradients of the log-likelihood are given by precisely this term.\n",
    "This fact makes computing gradients easy in practice.\n",
    "\n",
    "Now we can consider an entire distribution over outcomes, we now have a generic probability vector,\n",
    "say $(0.1, 0.2, 0.7)$. the loss $ l(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{j=1}^q y_j \\log \\hat{y}_j. $ still works well. It is the expected value of the loss for a distribution over labels.\n",
    "This loss is called the *cross-entropy loss* and it is\n",
    "one of the most commonly used losses for classification problems.\n",
    "\n",
    "## 2.1.3 Information Theory Basic\n",
    "\n",
    "\n",
    "### Entropy\n",
    "The central idea in information theory is to quantify the\n",
    "amount of information contained in data.\n",
    "This places a  limit on our ability to compress data.\n",
    "\n",
    "For a distribution $P$ its *entropy*, $H[P]$, is defined as:\n",
    "\n",
    "$$H[P] = \\sum_j - P(j) \\log P(j).$$\n",
    "\n",
    "One of the fundamental theorems of information theory states\n",
    "that in order to encode data drawn randomly from the distribution $P$,\n",
    "we need at least $H[P]$ \"nats\" to encode it. \"nat\" is the equivalent of bit\n",
    "but when using a code with base $e$ rather than one with base 2.\n",
    "Thus, one nat is $\\frac{1}{\\log(2)} \\approx 1.44$ bit.\n",
    "\n",
    "### Surprisal\n",
    "Claude Shannon settled on $\\log \\frac{1}{P(j)} = -\\log P(j)$\n",
    "to quantify one's *surprisal* at observing an event $j$\n",
    "having assigned it a (subjective) probability $P(j)$. The entropy $H[P] = \\sum_j - P(j) \\log P(j).$ is then the *expected surprisal*\n",
    "when one assigned the correct probabilities\n",
    "that truly match the data-generating process.\n",
    "\n",
    "### Cross-Entropy Revisited\n",
    "The cross-entropy *from* $P$ *to* $Q$, denoted $H(P, Q)$,\n",
    "is the expected surprisal of an observer with subjective probabilities $Q$\n",
    "upon seeing data that was actually generated according to probabilities $P$.\n",
    "This is given by $H(P, Q) \\stackrel{\\textrm{def}}{=} \\sum_j - P(j) \\log Q(j)$.\n",
    "The lowest possible cross-entropy is achieved when $P=Q$.\n",
    "In this case, the cross-entropy from $P$ to $Q$ is $H(P, P)= H(P)$.\n",
    "\n",
    "In short, we can think of the cross-entropy classification objective\n",
    "in two ways: (i) as maximizing the likelihood of the observed data;\n",
    "and (ii) as minimizing our surprisal (and thus the number of bits)\n",
    "required to communicate the labels.\n",
    "\n",
    "\n",
    "## 2.1.4 Summary\n",
    "We encountered the softmax,\n",
    "a convenient activation function that transforms\n",
    "outputs of an ordinary neural network layer\n",
    "into valid discrete probability distributions.\n",
    "We saw that the derivative of the cross-entropy loss\n",
    "when combined with softmax\n",
    "behaves very similarly\n",
    "to the derivative of squared error;\n",
    "\n",
    "Among other things, we skipped over computational considerations.\n",
    "Specifically, for any fully connected layer with $d$ inputs and $q$ outputs,\n",
    "the parametrization and computational cost is $\\mathcal{O}(dq)$,\n",
    "which can be prohibitively high in practice.\n",
    "Fortunately, this cost of transforming $d$ inputs into $q$ outputs\n",
    "can be reduced through approximation and compression.\n",
    "For instance Deep Fried Convnets\n",
    "uses a combination of permutations,\n",
    "Fourier transforms, and scaling\n",
    "to reduce the cost from quadratic to log-linear.\n",
    "Similar techniques work for more advanced\n",
    "structural matrix approximations.\n",
    "Lastly, we can use quaternion-like decompositions\n",
    "to reduce the cost to $\\mathcal{O}(\\frac{dq}{n})$,\n",
    "again if we are willing to trade off a small amount of accuracy\n",
    "for computational and storage cost\n",
    "based on a compression factor $n$.\n",
    "This is an active area of research.\n",
    "What makes it challenging is that\n",
    "we do not necessarily strive\n",
    "for the most compact representation\n",
    "or the smallest number of floating point operations\n",
    "but rather for the solution\n",
    "that can be executed most efficiently on modern GPUs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
