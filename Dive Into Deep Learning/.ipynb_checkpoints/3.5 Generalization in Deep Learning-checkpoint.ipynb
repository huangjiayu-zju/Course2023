{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f83cd917",
   "metadata": {},
   "source": [
    "# 3.5 Generalization in Deep Learning\n",
    "\n",
    "\n",
    "## 3.5.1 Revisiting Overfitting and Regularization\n",
    "With machine learning models encoding inductive biases,\n",
    "our approach to training them\n",
    "typically consists of two phases: (i) fit the training data;\n",
    "and (ii) estimate the *generalization error*\n",
    "(the true error on the underlying population)\n",
    "by evaluating the model on holdout data.\n",
    "The difference between our fit on the training data\n",
    "and our fit on the test data is called the *generalization gap* and when this is large,\n",
    "we say that our models *overfit* to the training data.\n",
    "And in the classical view,\n",
    "the interpretation is that our models are too complex,\n",
    "requiring that we either shrink the number of features,\n",
    "the number of nonzero parameters learned,\n",
    "or the size of the parameters as quantified.\n",
    "\n",
    "However deep learning complicates this picture in counterintuitive ways.\n",
    "First, for classification problems,\n",
    "our models are typically expressive enough\n",
    "to perfectly fit every training example,\n",
    "even in datasets consisting of millions.\n",
    "In the classical picture, we might think\n",
    "that this setting lies on the far right extreme\n",
    "of the model complexity axis,\n",
    "and that any improvements in generalization error\n",
    "must come by way of regularization. But that is where things start to get weird.\n",
    "\n",
    "Strangely, for many deep learning tasks\n",
    "(e.g., image recognition and text classification)\n",
    "we are typically choosing among model architectures,\n",
    "all of which can achieve arbitrarily low training loss\n",
    "(and zero training error).\n",
    "Because all models under consideration achieve zero training error,\n",
    "*the only avenue for further gains is to reduce overfitting*.\n",
    "Even stranger, it is often the case that\n",
    "despite fitting the training data perfectly,\n",
    "we can actually *reduce the generalization error*\n",
    "further by making the model *even more expressive*,\n",
    "e.g., adding layers, nodes, or training\n",
    "for a larger number of epochs.\n",
    "Stranger yet, the pattern relating the generalization gap\n",
    "to the *complexity* of the model (as captured, for example, in the depth or width of the networks)\n",
    "can be non-monotonic,\n",
    "with greater complexity hurting at first\n",
    "but subsequently helping in a so-called \"double-descent\" pattern.\n",
    "Thus the deep learning practitioner possesses a bag of tricks,\n",
    "some of which seemingly restrict the model in some fashion\n",
    "and others that seemingly make it even more expressive,\n",
    "and all of which, in some sense, are applied to mitigate overfitting.\n",
    "\n",
    "Complicating things even further,\n",
    "while the guarantees provided by classical learning theory\n",
    "can be conservative even for classical models,\n",
    "they appear powerless to explain why it is\n",
    "that deep neural networks generalize in the first place.\n",
    "Because deep neural networks are capable of fitting\n",
    "arbitrary labels even for large datasets,\n",
    "and despite the use of familiar methods such as $\\ell_2$ regularization,\n",
    "traditional complexity-based generalization bounds,\n",
    "e.g., those based on the VC dimension\n",
    "or Rademacher complexity of a hypothesis class\n",
    "cannot explain why neural networks generalize.\n",
    "\n",
    "## 3.5.2 Inspiration from Nonparametrics\n",
    "\n",
    "While neural networks clearly *have* parameters,\n",
    "in some ways it can be more fruitful\n",
    "to think of them as behaving like nonparametric models. One common theme is that nonparametric methods\n",
    "tend to have a level of complexity that grows\n",
    "as the amount of available data grows.\n",
    "\n",
    "In a sense, because neural networks are over-parametrized,\n",
    "possessing many more parameters than are needed to fit the training data,\n",
    "they tend to *interpolate* the training data (fitting it perfectly)\n",
    "and thus behave, in some ways, more like nonparametric models. More recent theoretical research has established\n",
    "deep connection between large neural networks\n",
    "and nonparametric methods, notably kernel methods.\n",
    "\n",
    "## 3.5.3 Early Stopping\n",
    "\n",
    "*Early stopping* is a classic technique for regularizing deep neural networks.\n",
    "Here, rather than directly constraining the values of the weights,\n",
    "one constrains the number of epochs of training.\n",
    "The most common way to determine the stopping criterion\n",
    "is to monitor validation error throughout training\n",
    "(typically by checking once after each epoch)\n",
    "and to cut off training when the validation error\n",
    "has not decreased by more than some small amount $\\epsilon$\n",
    "for some number of epochs.\n",
    "This is sometimes called a *patience criterion*.\n",
    "As well as the potential to lead to better generalization\n",
    "in the setting of noisy labels,\n",
    "another benefit of early stopping is the time saved.\n",
    "Once the patience criterion is met, one can terminate training.\n",
    "\n",
    "Notably, when there is no label noise and datasets are *realizable*\n",
    "(the classes are truly separable, e.g., distinguishing cats from dogs),\n",
    "early stopping tends not to lead to significant improvements in generalization.\n",
    "On the other hand, when there is label noise,\n",
    "or intrinsic variability in the label\n",
    "(e.g., predicting mortality among patients),\n",
    "early stopping is crucial.\n",
    "\n",
    "## 3.5.4 Classical Regularization Methods for Deep Networks\n",
    "\n",
    "Weight decay consists of adding a regularization term to the loss function\n",
    "in order to penalize large values of the weights.\n",
    "Depending on which weight norm is penalized\n",
    "this technique is known either as ridge regularization (for $\\ell_2$ penalty)\n",
    "or lasso regularization (for an $\\ell_1$ penalty).\n",
    "In the classical analysis of these regularizers,\n",
    "they are considered as sufficiently restrictive on the values\n",
    "that the weights can take to prevent the model from fitting arbitrary labels.\n",
    "\n",
    "However, in deep learning implementations, researchers have noted\n",
    "that typical strengths of $\\ell_2$ regularization\n",
    "are insufficient to prevent the networks\n",
    "from interpolating the data and thus the benefits if interpreted\n",
    "as regularization might only make sense\n",
    "in combination with the early stopping criterion. Absent early stopping, it is possible\n",
    "that just like the number of layers\n",
    "or number of nodes (in deep learning)\n",
    "or the distance metric (in 1-nearest neighbor),\n",
    "these methods may lead to better generalization\n",
    "not because they meaningfully constrain\n",
    "the power of the neural network\n",
    "but rather because they somehow encode inductive biases\n",
    "that are better compatible with the patterns\n",
    "found in datasets of interests.\n",
    "\n",
    "Notably, deep learning researchers have also built on techniques first popularized in classical regularization contexts, such as adding noise to model inputs. The famous dropout technique has become a mainstay of deep learning.\n",
    "\n",
    "## 3.5.5 Summary\n",
    "\n",
    "Unlike classical linear models,\n",
    "which tend to have fewer parameters than examples,\n",
    "deep networks tend to be over-parametrized,\n",
    "and for most tasks are capable\n",
    "of perfectly fitting the training set.\n",
    "This *interpolation regime* challenges\n",
    "many hard fast-held intuitions.\n",
    "Functionally, neural networks look like parametric models.\n",
    "But thinking of them as nonparametric models\n",
    "can sometimes be a more reliable source of intuition.\n",
    "Because it is often the case that all deep networks under consideration\n",
    "are capable of fitting all of the training labels,\n",
    "nearly all gains must come by mitigating overfitting\n",
    "(closing the *generalization gap*).\n",
    "Paradoxically, the interventions\n",
    "that reduce the generalization gap\n",
    "sometimes appear to increase model complexity\n",
    "and at other times appear to decrease complexity.\n",
    "However, these methods seldom decrease complexity\n",
    "sufficiently for classical theory\n",
    "to explain the generalization of deep networks,\n",
    "and *why certain choices lead to improved generalization*\n",
    "remains for the most part a massive open question\n",
    "despite the concerted efforts of many brilliant researchers.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
