{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ebb0efa",
   "metadata": {},
   "source": [
    "# 1.5 Generalization\n",
    "\n",
    "As machine learning scientists,\n",
    "our goal is to discover *patterns*. This problem---how to discover patterns that *generalize*---is\n",
    "the fundamental problem of machine learning,\n",
    "and arguably of all of statistics.\n",
    "\n",
    "The phenomenon of fitting closer to our training data\n",
    "than to the underlying distribution is called *overfitting*,\n",
    "and techniques for combatting overfitting\n",
    "are often called *regularization* methods.\n",
    "\n",
    "## 1.5.1 Training Error and Generalization Error\n",
    "\n",
    "In the standard supervised learning setting,\n",
    "we assume that the training data and the test data\n",
    "are drawn *independently* from *identical* distributions.\n",
    "This is commonly called the *IID assumption*.\n",
    "While this assumption is strong,\n",
    "it is worth noting that, absent any such assumption,\n",
    "we would be dead in the water.\n",
    "\n",
    "To begin with, we need to differentiate between\n",
    "the *training error* $R_\\textrm{emp}$,\n",
    "which is a *statistic*\n",
    "calculated on the training dataset,\n",
    "and the *generalization error* $R$,\n",
    "which is an *expectation* taken\n",
    "with respect to the underlying distribution.\n",
    "\n",
    "Formally the training error is expressed as a *sum*: $R_\\textrm{emp}[\\mathbf{X}, \\mathbf{y}, f]$ : 训练误差，是基于训练数据集的平均损失。$l(\\mathbf{x}^{(i)}, y^{(i)}, f(\\mathbf{x}^{(i)}))$: 在特定数据点$\\mathbf{x}^{(i)}$上的损失函数，$y^{(i)}$是实际标签，$f(\\mathbf{x}^{(i)})$是模型的预测\n",
    "\n",
    "$$R_\\textrm{emp}[\\mathbf{X}, \\mathbf{y}, f] = \\frac{1}{n} \\sum_{i=1}^n l(\\mathbf{x}^{(i)}, y^{(i)}, f(\\mathbf{x}^{(i)})),$$\n",
    "\n",
    "\n",
    "while the generalization error is expressed as an integral: $R[p, f]$: 泛化误差，即模型在整个数据分布上的期望损失。\n",
    "\n",
    "$$R[p, f] = E_{(\\mathbf{x}, y) \\sim P} [l(\\mathbf{x}, y, f(\\mathbf{x}))] =\n",
    "\\int \\int l(\\mathbf{x}, y, f(\\mathbf{x})) p(\\mathbf{x}, y) \\;d\\mathbf{x} dy.$$\n",
    "\n",
    "Problematically, we can never calculate\n",
    "the generalization error $R$ exactly.\n",
    "Nobody ever tells us the precise form\n",
    "of the density function $p(\\mathbf{x}, y)$.\n",
    "Moreover, we cannot sample an infinite stream of data points.\n",
    "Thus, in practice, we must *estimate* the generalization error\n",
    "by applying our model to an independent test set\n",
    "constituted of a random selection of examples\n",
    "$\\mathbf{X}'$ and labels $\\mathbf{y}'$\n",
    "that were withheld from our training set.\n",
    "\n",
    "Note that the model we wind up with\n",
    "depends explicitly on the selection of the training set\n",
    "and thus the training error will in general\n",
    "be a biased estimate of the true error\n",
    "on the underlying population.\n",
    "The central question of generalization\n",
    "is then when should we expect our training error\n",
    "to be close to the population error\n",
    "(and thus the generalization error).\n",
    "\n",
    "###  Model Complexity\n",
    "In classical theory, when we have\n",
    "simple models and abundant data,\n",
    "the training and generalization errors tend to be close.\n",
    "However, when we work with\n",
    "more complex models and/or fewer examples,\n",
    "we expect the training error to go down\n",
    "but the generalization gap to grow.\n",
    "\n",
    "When a model is capable of fitting arbitrary labels,\n",
    "low training error does not necessarily\n",
    "imply low generalization error.\n",
    "*However, it does not necessarily\n",
    "imply high generalization error either!*\n",
    "All we can say with confidence is that\n",
    "low training error alone is not enough\n",
    "to certify low generalization error.\n",
    "Deep neural networks turn out to be just such models::\n",
    "while they generalize well in practice,\n",
    "they are too powerful to allow us to conclude\n",
    "much on the basis of training error alone. In these cases we must rely more heavily\n",
    "on our holdout data to certify generalization\n",
    "after the fact.\n",
    "Error on the holdout data, i.e., validation set,\n",
    "is called the *validation error*.\n",
    "\n",
    "## 1.5.2 Underfitting or Overfitting?\n",
    "When our training error and validation error are both substantial\n",
    "but there is a little gap between them.\n",
    "If the model is unable to reduce the training error,\n",
    "that could mean that our model is too simple to capture the pattern that we are trying to model.\n",
    "Moreover, since the *generalization gap* ($R_\\textrm{emp} - R$)\n",
    "between our training and generalization errors is small,\n",
    "we have reason to believe that we could get away with a more complex model.\n",
    "This phenomenon is known as *underfitting*.\n",
    "\n",
    "When our training error is significantly lower\n",
    "than our validation error, indicating severe *overfitting*.\n",
    "Note that overfitting is not always a bad thing.\n",
    "In deep learning especially,the best predictive models often perform\n",
    "far better on training data than on holdout data.\n",
    "Ultimately, we usually care about\n",
    "driving the generalization error lower,\n",
    "and only care about the gap insofar\n",
    "as it becomes an obstacle to that end.\n",
    "\n",
    "### Polynomial Curve Fitting\n",
    "A higher-order polynomial function is more complex\n",
    "than a lower-order polynomial function,\n",
    "since the higher-order polynomial has more parameters\n",
    "and the model function's selection range is wider.\n",
    "Fixing the training dataset,\n",
    "higher-order polynomial functions should always\n",
    "achieve lower (at worst, equal) training error\n",
    "relative to lower-degree polynomials.\n",
    "In fact, whenever each data example\n",
    "has a distinct value of $x$,\n",
    "a polynomial function with degree\n",
    "equal to the number of data examples\n",
    "can fit the training set perfectly.\n",
    "\n",
    "### Dataset Size\n",
    "Fixing our model, the fewer samples\n",
    "we have in the training dataset,\n",
    "the more likely (and more severely)\n",
    "we are to encounter overfitting.\n",
    "As we increase the amount of training data,\n",
    "the generalization error typically decreases.\n",
    "Moreover, in general, more data never hurts.\n",
    "For a fixed task and data distribution,\n",
    "model complexity should not increase\n",
    "more rapidly than the amount of data.\n",
    "Given more data, we might  attempt\n",
    "to fit a more complex model.Absent sufficient data, simpler models\n",
    "may be more difficult to beat.\n",
    "For many tasks, deep learning\n",
    "only outperforms linear models\n",
    "when many thousands of training examples are available.\n",
    "In part, the current success of deep learning\n",
    "owes considerably to the abundance of massive datasets\n",
    "arising from Internet companies, cheap storage,\n",
    "connected devices, and the broad digitization of the economy.\n",
    "\n",
    "## 1.5.3 Model Selection\n",
    "\n",
    "Typically, we select our final model\n",
    "only after evaluating multiple models\n",
    "that differ in various ways\n",
    "(different architectures, training objectives,\n",
    "selected features, data preprocessing,\n",
    "learning rates, etc.).\n",
    "Choosing among many models is aptly\n",
    "called *model selection*.\n",
    "\n",
    "In principle, we should not touch our test set\n",
    "until after we have chosen all our hyperparameters.Were we to use the test data in the model selection process,\n",
    "there is a risk that we might overfit the test data.\n",
    "\n",
    "Thus, we should never rely on the test data for model selection. And yet we cannot rely solely on the training data for model selection either because we cannot estimate the generalization error on the very data that we use to train the model.\n",
    "\n",
    "The common practice for addressing the problem of *training on the test set*\n",
    "is to split our data three ways,\n",
    "incorporating a *validation set*\n",
    "in addition to the training and test datasets. Unless explicitly stated otherwise, in the experiments in this book\n",
    "we are really working with what should rightly be called\n",
    "training data and validation data, with no true test sets.\n",
    "Therefore, the accuracy reported in each experiment of the book is really\n",
    "the validation accuracy and not a true test set accuracy.\n",
    "\n",
    "### Cross-Validation\n",
    "When training data is scarce,\n",
    "we might not even be able to afford to hold out\n",
    "enough data to constitute a proper validation set.\n",
    "One popular solution to this problem is to employ\n",
    "$K$*-fold cross-validation*.\n",
    "Here, the original training data is split into $K$ non-overlapping subsets.\n",
    "Then model training and validation are executed $K$ times,\n",
    "each time training on $K-1$ subsets and validating\n",
    "on a different subset (the one not used for training in that round).\n",
    "Finally, the training and validation errors are estimated\n",
    "by averaging over the results from the $K$ experiments.\n",
    "\n",
    "## 1.5.4 Summary\n",
    "We leave you with a few rules of thumb:\n",
    "\n",
    "1. Use validation sets (or $K$*-fold cross-validation*) for model selection;\n",
    "1. More complex models often require more data;\n",
    "1. Relevant notions of complexity include both the number of parameters and the range of values that they are allowed to take;\n",
    "1. Keeping all else equal, more data almost always leads to better generalization;\n",
    "1. This entire talk of generalization is all predicated on the IID assumption. If we relax this assumption, allowing for distributions to shift between the train and testing periods, then we cannot say anything about generalization absent a further (perhaps milder) assumption."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
