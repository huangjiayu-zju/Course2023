{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6c313f0",
   "metadata": {},
   "source": [
    "# 5.1 From Fully Connected Layers to Convolutions\n",
    "\n",
    "For high-dimensional perceptual data, such structureless networks like MLP can grow unwieldy. Assume we are collecting an annotated dataset of one-megapixel photographs, this means that each input to the network has one million dimensions.\n",
    "Even an aggressive reduction to one thousand hidden dimensions\n",
    "would require a fully connected layer\n",
    "characterized by $10^6 \\times 10^3 = 10^9$ parameters.Unless we have lots of GPUs, a talent\n",
    "for distributed optimization,\n",
    "and an extraordinary amount of patience,\n",
    "learning the parameters of this network\n",
    "may turn out to be infeasible. Our hidden layer of size 1000 grossly underestimates\n",
    "the number of hidden units that it takes\n",
    "to learn good representations of images,\n",
    "so a practical system will still require billions of parameters.\n",
    "Moreover, learning a classifier by fitting so many parameters\n",
    "might require collecting an enormous dataset.\n",
    "\n",
    "Images exhibit rich structure\n",
    "that can be exploited by humans\n",
    "and machine learning models alike.\n",
    "Convolutional neural networks (CNNs) are one creative way\n",
    "that machine learning has embraced for exploiting\n",
    "some of the known structure in natural images.\n",
    "\n",
    "\n",
    "## 5.1.1 Invariance\n",
    "Imagine that we want to detect an object in an image.\n",
    "It seems reasonable that whatever method\n",
    "we use to recognize objects should not be overly concerned\n",
    "with the precise location of the object in the image. *what Waldo looks like*\n",
    "does not depend upon *where Waldo is located*.\n",
    "\n",
    "We can enumerate a few desiderata to guide our design of a neural network architecture suitable for computer vision:\n",
    "1. In the earliest layers, our network\n",
    "   should respond similarly to the same patch,\n",
    "   regardless of where it appears in the image. This principle is called *translation invariance* (or *translation equivariance*).\n",
    "1. The earliest layers of the network should focus on local regions,\n",
    "   without regard for the contents of the image in distant regions. This is the *locality* principle.\n",
    "   Eventually, these local representations can be aggregated\n",
    "   to make predictions at the whole image level.\n",
    "1. As we proceed, deeper layers should be able to capture longer-range features of the \n",
    "   image, in a way similar to higher level vision in nature. \n",
    "   \n",
    "## 5.1.2 Constraining the MLP\n",
    "\n",
    "To start off, we can consider an MLP\n",
    "with two-dimensional images $\\mathbf{X}$ as inputs\n",
    "and their immediate hidden representations\n",
    "$\\mathbf{H}$ similarly represented as matrices (they are two-dimensional tensors in code), where both $\\mathbf{X}$ and $\\mathbf{H}$ have the same shape.\n",
    "\n",
    "Let $[\\mathbf{X}]_{i, j}$ and $[\\mathbf{H}]_{i, j}$ denote the pixel\n",
    "at location $(i,j)$\n",
    "in the input image and hidden representation, respectively.\n",
    "Consequently, to have each of the hidden units\n",
    "receive input from each of the input pixels,\n",
    "we would switch from using weight matrices\n",
    "(as we did previously in MLPs)\n",
    "to representing our parameters\n",
    "as fourth-order weight tensors $\\mathsf{W}$.\n",
    "Suppose that $\\mathbf{U}$ contains biases,\n",
    "we could formally express the fully connected layer as\n",
    "\n",
    "$$\\begin{aligned} \\left[\\mathbf{H}\\right]_{i, j} &= [\\mathbf{U}]_{i, j} + \\sum_k \\sum_l[\\mathsf{W}]_{i, j, k, l}  [\\mathbf{X}]_{k, l}\\\\ &=  [\\mathbf{U}]_{i, j} +\n",
    "\\sum_a \\sum_b [\\mathsf{V}]_{i, j, a, b}  [\\mathbf{X}]_{i+a, j+b}.\\end{aligned}$$\n",
    "\n",
    "\n",
    "$[\\mathsf{V}]_{i, j, a, b}$ is a fourth-order tensor where $i, j$ describe a location in the image, and $a, b$ describe an offset from that location. Given $a, b \\in (-1000, 1000)$, without other constraints, the number of parameters would be $10^{12}$.\n",
    "\n",
    "### Translation Invariance\n",
    "\n",
    "A shift in the input $\\mathbf{X}$\n",
    "should simply lead to a shift in the hidden representation $\\mathbf{H}$.\n",
    "This is only possible if $\\mathsf{V}$ and $\\mathbf{U}$ do not actually depend on $(i, j)$. As such,\n",
    "we have $[\\mathsf{V}]_{i, j, a, b} = [\\mathbf{V}]_{a, b}$ and $\\mathbf{U}$ is a constant, say $u$.\n",
    "As a result, we can simplify the definition for $\\mathbf{H}$:\n",
    "\n",
    "$$[\\mathbf{H}]_{i, j} = u + \\sum_a\\sum_b [\\mathbf{V}]_{a, b}  [\\mathbf{X}]_{i+a, j+b}.$$\n",
    "\n",
    "\n",
    "This is a *convolution*!\n",
    "We are effectively weighting pixels at $(i+a, j+b)$\n",
    "in the vicinity of location $(i, j)$ with coefficients $[\\mathbf{V}]_{a, b}$\n",
    "to obtain the value $[\\mathbf{H}]_{i, j}$.\n",
    "\n",
    "By considering translational invariance in images, which means treating a feature as equally important anywhere in the image, the weights $[\\mathbf{V}]_{a, b}$ no longer depend on the specific location $i, j$. This allows us to reduce the number of parameters.\n",
    "\n",
    "Given $a, b \\in (-1000, 1000)$, each dimension has $2001$ possible values (ranging from -1000 to 1000, inclusive).\n",
    "Thus, the total parameter count is: $2001 \\times 2001 = 4,004,001$, which is approximately $4 \\times 10^6$.\n",
    "By taking advantage of translational invariance in images, the number of parameters was significantly reduced, from $10^{12}$ down to around $4 \\times 10^6$.\n",
    "\n",
    "### Locality\n",
    "\n",
    "Now let's invoke the second principle: locality.\n",
    "As motivated above, we believe that we should not have\n",
    "to look very far away from location $(i, j)$\n",
    "in order to glean relevant information\n",
    "to assess what is going on at $[\\mathbf{H}]_{i, j}$.\n",
    "This means that outside some range $|a|> \\Delta$ or $|b| > \\Delta$,\n",
    "we should set $[\\mathbf{V}]_{a, b} = 0$.\n",
    "Equivalently, we can rewrite $[\\mathbf{H}]_{i, j}$ as\n",
    "\n",
    "$$[\\mathbf{H}]_{i, j} = u + \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} [\\mathbf{V}]_{a, b}  [\\mathbf{X}]_{i+a, j+b}.$$\n",
    "\n",
    "This reduces the number of parameters from $4 \\times 10^6$ to $4 \\Delta^2$, where $\\Delta$ is typically smaller than $10$. As such, we reduced the number of parameters by another four orders of magnitude. The above formula is what is called, in a nutshell, a *convolutional layer*. \n",
    "*Convolutional neural networks* (CNNs)\n",
    "are a special family of neural networks that contain convolutional layers.\n",
    "In the deep learning research community,\n",
    "$\\mathbf{V}$ is referred to as a *convolution kernel*,\n",
    "a *filter*, or simply the layer's *weights* that are learnable parameters.\n",
    "\n",
    "This dramatic reduction in parameters brings us to our last desideratum, namely that deeper layers should represent larger and more complex aspects of an image. This can be achieved by interleaving nonlinearities and convolutional layers repeatedly.\n",
    "\n",
    "## 5.1.3 Concolutions\n",
    "\n",
    "In mathematics, the *convolution* between two functions,\n",
    "say $f, g: \\mathbb{R}^d \\to \\mathbb{R}$ is defined as\n",
    "\n",
    "$$(f * g)(\\mathbf{x}) = \\int f(\\mathbf{z}) g(\\mathbf{x}-\\mathbf{z}) d\\mathbf{z}.$$\n",
    "\n",
    "That is, we measure the overlap between $f$ and $g$\n",
    "when one function is \"flipped\" and shifted by $\\mathbf{x}$.\n",
    "Whenever we have discrete objects, the integral turns into a sum.\n",
    "\n",
    "For two-dimensional tensors, we have a corresponding sum\n",
    "with indices $(a, b)$ for $f$ and $(i-a, j-b)$ for $g$, respectively:\n",
    "\n",
    "$$(f * g)(i, j) = \\sum_a\\sum_b f(a, b) g(i-a, j-b).$$\n",
    "\n",
    "## 5.1.4 Channels\n",
    "\n",
    "So far, we blissfully ignored that images consist\n",
    "of three channels: red, green, and blue. While the first two of these axes concern spatial relationships,\n",
    "the third can be regarded as assigning\n",
    "a multidimensional representation to each pixel location.\n",
    "We thus index $\\mathsf{X}$ as $[\\mathsf{X}]_{i, j, k}$.\n",
    "The convolutional filter has to adapt accordingly.\n",
    "Instead of $[\\mathbf{V}]_{a,b}$, we now have $[\\mathsf{V}]_{a,b,c}$.\n",
    "\n",
    "We want an entire vector of hidden representations\n",
    "corresponding to each spatial location.\n",
    "We could think of the hidden representations as comprising\n",
    "a number of two-dimensional grids stacked on top of each other.\n",
    "As in the inputs, these are sometimes called *channels*.\n",
    "They are also sometimes called *feature maps*,\n",
    "as each provides a spatialized set\n",
    "of learned features for the subsequent layer.To support multiple channels in both inputs ($\\mathsf{X}$) and hidden representations ($\\mathsf{H}$),\n",
    "we can add a fourth coordinate to $\\mathsf{V}$: $[\\mathsf{V}]_{a, b, c, d}$.\n",
    "Putting everything together we have:\n",
    "\n",
    "$$[\\mathsf{H}]_{i,j,d} = \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} \\sum_c [\\mathsf{V}]_{a, b, c, d} [\\mathsf{X}]_{i+a, j+b, c},$$\n",
    "\n",
    "where $d$ indexes the output channels in the hidden representations $\\mathsf{H}$. The subsequent convolutional layer will go on to take a third-order tensor, $\\mathsf{H}$, as input.\n",
    "We take it as the definition of a convolutional layer for multiple channels, where $\\mathsf{V}$ is a kernel or filter of the layer.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
