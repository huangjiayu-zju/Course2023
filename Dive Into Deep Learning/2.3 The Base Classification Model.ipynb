{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "204fbadf",
   "metadata": {},
   "source": [
    "# 2.3 The Base Classification Model\n",
    "\n",
    "This section provides a base class for classification models to simplify future code.\n",
    "\n",
    "## 2.3.1 The `Classifier` Class\n",
    "\n",
    "We define the `Classifier` class below. In the `validation_step` we report both the loss value and the classification accuracy on a validation batch. We draw an update for every `num_val_batches` batches. This has the benefit of generating the averaged loss and accuracy on the whole validation data. These average numbers are not exactly correct if the final batch contains fewer examples, but we ignore this minor difference to keep the code simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a2fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from d2l import torch as d2l\n",
    "\n",
    "class Classifier(d2l.Module):  #@save\n",
    "    \"\"\"The base class of classification models.\"\"\"\n",
    "    def validation_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1]) # 对输入数据进行前向传播得到预测\n",
    "        self.plot('loss', self.loss(Y_hat, batch[-1]), train=False) # 计算损失并绘制到验证集损失图上\n",
    "        self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=False) # 计算准确率并绘制到验证集准确率图上"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3d1af4",
   "metadata": {},
   "source": [
    "By default we use a stochastic gradient descent optimizer, operating on minibatches, just as we did in the context of linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "800b8607",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.Module)  #@save\n",
    "def configure_optimizers(self):\n",
    "    return torch.optim.SGD(self.parameters(), lr=self.lr) # 使用随机梯度下降作为优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c5e243",
   "metadata": {},
   "source": [
    "## 2.3.2 Accuracy\n",
    "\n",
    "When predictions are consistent with the label class `y`, they are correct.\n",
    "The classification accuracy is the fraction of all predictions that are correct.\n",
    "\n",
    "Accuracy is computed as follows.\n",
    "First, if `y_hat` is a matrix,\n",
    "we assume that the second dimension stores prediction scores for each class.\n",
    "We use `argmax` to obtain the predicted class by the index for the largest entry in each row.\n",
    "Then we compare the predicted class with the ground truth `y` elementwise.\n",
    "Since the equality operator `==` is sensitive to data types,\n",
    "we convert `y_hat`'s data type to match that of `y`.\n",
    "The result is a tensor containing entries of 0 (false) and 1 (true).\n",
    "Taking the sum yields the number of correct predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e8cc64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算模型的预测精确度。\n",
    "@d2l.add_to_class(Classifier)  #@save\n",
    "def accuracy(self, Y_hat, Y, averaged=True):\n",
    "    \"\"\"Compute the number of correct predictions.\"\"\"\n",
    "    Y_hat = Y_hat.reshape((-1, Y_hat.shape[-1])) # 重新调整Y_hat的形状，使其成为二维张量\n",
    "    preds = Y_hat.argmax(axis=1).type(Y.dtype)   # 使用argmax获取每行的最大值的索引，这代表模型的预测类别\n",
    "    compare = (preds == Y.reshape(-1)).type(torch.float32) # 比较预测值和真实标签，看它们是否相等\n",
    "    return compare.mean() if averaged else compare  # 如果averaged=True，则返回平均精确度；否则，返回每个样本的比较结果"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
