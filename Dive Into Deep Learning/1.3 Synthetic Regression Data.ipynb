{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44f75142",
   "metadata": {},
   "source": [
    "# 1.3 Synthetic Regression Data\n",
    "\n",
    "\n",
    "## 1.3.1 Generating the Dataset\n",
    "\n",
    "The following code snippet generates 1000 examples\n",
    "with 2-dimensional features drawn \n",
    "from a standard normal distribution.\n",
    "The resulting design matrix $\\mathbf{X}$\n",
    "belongs to $\\mathbb{R}^{1000 \\times 2}$. \n",
    "We generate each label by applying \n",
    "a *ground truth* linear function, \n",
    "corrupting them via additive noise $\\boldsymbol{\\epsilon}$, \n",
    "drawn independently and identically for each example:\n",
    "\n",
    "$$\\mathbf{y}= \\mathbf{X} \\mathbf{w} + b + \\boldsymbol{\\epsilon}.$$\n",
    "\n",
    "For convenience we assume that $\\boldsymbol{\\epsilon}$ is drawn \n",
    "from a normal distribution with mean $\\mu= 0$ \n",
    "and standard deviation $\\sigma = 0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a788287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1a2ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticRegressionData(d2l.DataModule):  #@save\n",
    "    \"\"\"Synthetic data for linear regression.\"\"\"\n",
    "    def __init__(self, w, b, noise=0.01, num_train=1000, num_val=1000,\n",
    "                 batch_size=32):\n",
    "        super().__init__() # 调用父类的初始化方法\n",
    "        self.save_hyperparameters() # 保存超参数\n",
    "        n = num_train + num_val # 计算样本总数\n",
    "        self.X = torch.randn(n, len(w))   # 生成随机特征数据（大小为 n x 特征数）\n",
    "        noise = torch.randn(n, 1) * noise # 生成加性噪声（大小为 n x 1）\n",
    "        self.y = torch.matmul(self.X, w.reshape((-1, 1))) + b + noise  # 计算标签数据，通过特征和权重相乘并加上偏置和噪声"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405624c7",
   "metadata": {},
   "source": [
    "Below, we set the true parameters to $\\mathbf{w} = [2, -3.4]^\\top$ and $b = 4.2$.\n",
    "Later, we can check our estimated parameters against these *ground truth* values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8703b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个 SyntheticRegressionData 实例，设置真实参数 w 和 b\n",
    "data = SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2ceef4",
   "metadata": {},
   "source": [
    "Each row in `features` consists of a vector in $\\mathbb{R}^2$ and each row in `labels` is a scalar. Let's have a look at the first entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f6534d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: tensor([-0.1407,  0.1343]) \n",
      "label: tensor([3.4590])\n"
     ]
    }
   ],
   "source": [
    "# 打印第一个样本的特征和标签\n",
    "print('features:', data.X[0],'\\nlabel:', data.y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d57716",
   "metadata": {},
   "source": [
    "## 1.3.2 Reading the Dataset\n",
    "Training machine learning models often requires multiple passes over a dataset, \n",
    "grabbing one minibatch of examples at a time. \n",
    "This data is then used to update the model. \n",
    "To illustrate how this works, we implement the `get_dataloader` method,\n",
    "registering it in the `SyntheticRegressionData` class via `add_to_class`.\n",
    "It takes a batch size, a matrix of features,\n",
    "and a vector of labels, and generates minibatches of size `batch_size`.\n",
    "As such, each minibatch consists of a tuple of features and labels. \n",
    "Note that we need to be mindful of whether we're in training or validation mode: \n",
    "in the former, we will want to read the data in random order, \n",
    "whereas for the latter, being able to read data in a pre-defined order \n",
    "may be important for debugging purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42aa0e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(SyntheticRegressionData)\n",
    "def get_dataloader(self, train): #生成数据加载器，支持训练模式和验证模式\n",
    "    if train:\n",
    "        indices = list(range(0, self.num_train)) # 如果是训练模式，创建一个包含训练数据索引的列表\n",
    "        # The examples are read in random order\n",
    "        random.shuffle(indices) # 训练模式下随机打乱数据顺序\n",
    "    else:\n",
    "        indices = list(range(self.num_train, self.num_train+self.num_val))\n",
    "    for i in range(0, len(indices), self.batch_size): # 使用 minibatch 大小迭代数据\n",
    "        batch_indices = torch.tensor(indices[i: i+self.batch_size]) # 获取当前 minibatch 的索引\n",
    "        yield self.X[batch_indices], self.y[batch_indices]  # 返回当前 minibatch 的特征和标签"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999d5b42",
   "metadata": {},
   "source": [
    "To build some intuition, let's inspect the first minibatch of\n",
    "data. Each minibatch of features provides us with both its size and the dimensionality of input features.\n",
    "Likewise, our minibatch of labels will have a matching shape given by `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26d62a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([32, 2]) \n",
      "y shape: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "X, y = next(iter(data.train_dataloader())) #获取第一个 minibatch 的数据\n",
    "print('X shape:', X.shape, '\\ny shape:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50ea354",
   "metadata": {},
   "source": [
    "Throughout the iteration we obtain distinct minibatches\n",
    "until the entire dataset has been exhausted (try this).\n",
    "While the iteration implemented above is good for didactic purposes,\n",
    "it is inefficient in ways that might get us into trouble with real problems.\n",
    "For example, it requires that we load all the data in memory\n",
    "and that we perform lots of random memory access.\n",
    "The built-in iterators implemented in a deep learning framework\n",
    "are considerably more efficient and they can deal\n",
    "with sources such as data stored in files, \n",
    "data received via a stream, \n",
    "and data generated or processed on the fly. \n",
    "Next let's try to implement the same method using built-in iterators.\n",
    "\n",
    "## 1.3.3 Concise Implementation of the Data Loader\n",
    "Rather than writing our own iterator,\n",
    "we can call the existing API in a framework to load data.\n",
    "As before, we need a dataset with features `X` and labels `y`. \n",
    "Beyond that, we set `batch_size` in the built-in data loader \n",
    "and let it take care of shuffling examples  efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8f3632a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.DataModule)  #@save\n",
    "def get_tensorloader(self, tensors, train, indices=slice(0, None)):\n",
    "    tensors = tuple(a[indices] for a in tensors)   # 根据指定的索引切片选择子集合\n",
    "    dataset = torch.utils.data.TensorDataset(*tensors) # 创建一个 PyTorch 的 TensorDataset\n",
    "    return torch.utils.data.DataLoader(dataset, self.batch_size,  # 使用 batch_size 和 shuffle 参数创建数据加载器\n",
    "                                       shuffle=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adba6626",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(SyntheticRegressionData)  #@save\n",
    "def get_dataloader(self, train):\n",
    "    i = slice(0, self.num_train) if train else slice(self.num_train, None)  # 根据是否是训练模式选择数据切片的范围\n",
    "    return self.get_tensorloader((self.X, self.y), train, i) # 使用 get_tensorloader 方法创建数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46dbfe42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([32, 2]) \n",
      "y shape: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "X, y = next(iter(data.train_dataloader()))\n",
    "print('X shape:', X.shape, '\\ny shape:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb2dfe1",
   "metadata": {},
   "source": [
    "The new data loader behaves just like the previous one, except that it is more efficient and has some added functionality.\n",
    "For instance, the data loader provided by the framework API \n",
    "supports the built-in `__len__` method, \n",
    "so we can query its length, \n",
    "i.e., the number of batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "087ef566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.train_dataloader()) # 获取数据加载器的长度"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
