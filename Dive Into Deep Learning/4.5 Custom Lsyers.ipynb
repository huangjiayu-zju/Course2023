{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fde50a3",
   "metadata": {},
   "source": [
    "# 4.5 Custom Layers\n",
    "\n",
    "## 4.5.1 Layers without Parameters\n",
    "\n",
    "To start, we construct a custom layer\n",
    "that does not have any parameters of its own.The following `CenteredLayer` class simply\n",
    "subtracts the mean from its input.\n",
    "To build it, we simply need to inherit\n",
    "from the base layer class and implement the forward propagation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1f0212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8410ad",
   "metadata": {},
   "source": [
    "Let's verify that our layer works as intended by feeding some data through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97453b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = CenteredLayer()\n",
    "layer(torch.tensor([1.0, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd33aba3",
   "metadata": {},
   "source": [
    "We can now incorporate our layer as a component in constructing more complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83a3697c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiayuhuang/anaconda3/envs/d2l/lib/python3.8/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(nn.LazyLinear(128), CenteredLayer())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba4ea93",
   "metadata": {},
   "source": [
    "As an extra sanity check, we can send random data\n",
    "through the network and check that the mean is in fact 0.\n",
    "Because we are dealing with floating point numbers,\n",
    "we may still see a very small nonzero number\n",
    "due to quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "038a93e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.7940e-09, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = net(torch.rand(4, 8))\n",
    "Y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8949cab",
   "metadata": {},
   "source": [
    "## 4.5.2 Layers with Parameters\n",
    "We can use built-in functions to create parameters, which\n",
    "provide some basic housekeeping functionality.\n",
    "In particular, they govern access, initialization,\n",
    "sharing, saving, and loading model parameters.\n",
    "This way, among other benefits, we will not need to write\n",
    "custom serialization routines for every custom layer.\n",
    "\n",
    "Now let's implement our own version of the  fully connected layer.\n",
    "Recall that this layer requires two parameters,\n",
    "one to represent the weight and the other for the bias.\n",
    "In this implementation, we bake in the ReLU activation as a default.\n",
    "This layer requires two input arguments: `in_units` and `units`, which\n",
    "denote the number of inputs and outputs, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e55d24e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_units, units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units, units))\n",
    "        self.bias = nn.Parameter(torch.randn(units,))\n",
    "\n",
    "    def forward(self, X):\n",
    "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
    "        return F.relu(linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d4c30f",
   "metadata": {},
   "source": [
    "Next, we instantiate the `MyLinear` class\n",
    "and access its model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f391fc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.1170, -0.4883,  1.8680],\n",
       "        [ 0.2612, -1.7494, -0.2981],\n",
       "        [-1.0007, -0.2627,  0.4486],\n",
       "        [ 1.4781,  0.4477,  0.7075],\n",
       "        [ 1.3846, -0.1300, -0.9291]], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = MyLinear(5, 3)\n",
    "linear.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54af2517",
   "metadata": {},
   "source": [
    "We can directly carry out forward propagation calculations using custom layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd794ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4056, 0.0000, 0.0000],\n",
       "        [3.7758, 0.0319, 0.0000]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(torch.rand(2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ad40ef",
   "metadata": {},
   "source": [
    "We can also construct models using custom layers.\n",
    "Once we have that we can use it just like the built-in fully connected layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00f61774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.7653],\n",
       "        [14.4193]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))\n",
    "net(torch.rand(2, 64))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
