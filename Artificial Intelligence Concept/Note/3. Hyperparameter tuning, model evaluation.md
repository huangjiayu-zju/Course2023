## 3. Hyperparameter tuning, model evaluation

[TOC]

Hyperparameter optimization, also known as hyperparameter tuning, is a crucial aspect of machine learning that involves selecting the optimal set of hyperparameters for a learning algorithm. Examples of hyperparameters include learning rate, regularization strength, the number of hidden layers in a neural network, and the number of trees in a random forest. 

Hyperparameter tuning is a challenging task as the optimal values are problem-specific and dependent on the available data. In practice, a variety of strategies are employed to tackle this issue. 
- One common approach is **grid search**, where the hyperparameter space is discretized, and a set of combinations is tested exhaustively. 
- Another approach is **random search**, where the hyperparameters are sampled randomly from a given distribution. 
- **Bayesian optimization** is another popular method that models the performance of the learning algorithm as a function of hyperparameters and updates the distribution over the hyperparameter space to minimize the expected loss.

#### Train a multiclass logistic regression classifier

Multinomial logistic regression is a well-known method for multiclass classification which provides a probabilistic framework for assigning class labels. It can handle cases where the classes are not linearly separable and allows for more complex decision boundaries in the feature space. This makes it suitable for a wide range of applications, including image recognition, text categorization, and multi-class sentiment analysis.

**Train a multinomial logistic regression model on the preprocessed data (scikit learn LogisticRegression())**: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html

The following are some important hyperparameters that can significantly influence the performance of the model:

- **regularization type (penalty = {'l1', 'l2', 'elasticnet', None}, default='l2')** 

- **regularization strength (C)**

- **class_weight**

- **solver algorithm (solver)** 

- **and maximum iterations (max_iter)** 

#### grid search

Instead of dividing the dataset into solely a train and test set, it is divided into three sets: a train set, a test set, and a validation set. The parameters are then tuned based on the validation set's performance in the cross-validation framework. The test set is reserved solely for reporting the model's accuracy on unseen data.

Scikit-learn provides functions for performing grid search: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV