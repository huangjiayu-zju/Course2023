## **Unsupervised Learning**

[TOC]

Many instances of unsupervised learning, such as dimensionality reduction, manifold learning, and feature extraction, find a new representation of the input data without any additional input.

#### **1. Data Transformation**

Rescaling falls into the category of data pre-processing, there exist many different rescaling technques, and we will take a look at a particular method that is commonly called "standardization." Here, we will recale the data so that each feature is centered at zero (mean = 0) with unit variance (standard deviation = 0).

the standardized values are computed via the equation $x_{standardized} = \frac{x - \mu_x}{\sigma_x}$,

where $\mu$ is the sample mean, and $\sigma$ the standard deviation, respectively. scikit-learn implements a `StandardScaler` class for this computation. 

#### **2. Dimentionality Redcution: Principal Component Analysis**

An unsupervised transformation that is somewhat more interesting is Principal Component Analysis (PCA).

It is a technique to reduce the dimensionality of the data, by creating a linear projection.

That is, we find new features to represent the data that are a linear combination of the old data. Thus, we can think of PCA as a projection of our data onto a **new** feature space.

The way PCA finds these new directions is by looking for the directions of maximum variance. Usually only few components that explain most of the variance in the data are kept. Here, the premise is to reduce the size (dimensionality) of a dataset while capturing most of its information. 

#### **3. Clustering**

Clustering is the task of gathering samples into groups of similar samples according to some predefined similarity or distance (dissimilarity) measure, such as the Euclidean distance.

K-means is an iterative algorithm which searches for k cluster centers such that the distance from each point to its cluster is minimized. The standard implementation of K-means uses the Euclidean distance, which is why we want to make sure that all our variables are measured on the same scale if we are working with real-world datastets.

Clustering comes with assumptions: A clustering algorithm finds clusters by making assumptions with samples should be grouped together. For K-means clustering, the model is that all clusters have equal, spherical variance.

#### **4. Some Notable Clustering Algorithms**

The following are some well-known clustering algorithms available in scikit-learn library:

\- `sklearn.cluster.KMeans`: <br/>

​    The simplest, yet effective clustering algorithm. Needs to be provided with the

​    number of clusters in advance, and assumes that the data is normalized as input

​    (but use a PCA model as preprocessor).

\- `sklearn.cluster.MeanShift`: <br/>

​    Can find better looking clusters than KMeans but is not scalable to high number of samples.

\- `sklearn.cluster.DBSCAN`: <br/>

​    Can detect irregularly shaped clusters based on density, i.e. sparse regions in

​    the input space are likely to become inter-cluster boundaries. Can also detect

​    outliers (samples that are not part of a cluster).

\- `sklearn.cluster.AffinityPropagation`: <br/>

​    Clustering algorithm based on message passing between data points.

\- `sklearn.cluster.SpectralClustering`: <br/>

​    KMeans applied to a projection of the normalized graph Laplacian: finds

​    normalized graph cuts if the affinity matrix is interpreted as an adjacency matrix of a graph.

\- `sklearn.cluster.Ward`: <br/>

​    Ward implements hierarchical clustering based on the Ward algorithm,

​    a variance-minimizing approach. At each step, it minimizes the sum of

​    squared differences within all clusters (inertia criterion).

Of these, Ward, SpectralClustering, DBSCAN and Affinity propagation can also work with precomputed similarity matrices.