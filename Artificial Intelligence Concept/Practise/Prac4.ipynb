{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c393db25",
      "metadata": {
        "id": "c393db25"
      },
      "source": [
        "## Hyperparameter tuning, model evaluation\n",
        "\n",
        "Hyperparameter optimization, also known as hyperparameter tuning, is a crucial aspect of machine learning that involves selecting the optimal set of hyperparameters for a learning algorithm. Hyperparameters are parameters that control the learning process and are typically set before the training begins. Examples of hyperparameters include learning rate, regularization strength, the number of hidden layers in a neural network, and the number of trees in a random forest. The performance of a model can vary significantly depending on the values of these hyperparameters.\n",
        "\n",
        "Hyperparameter tuning is a challenging task as the optimal values are problem-specific and dependent on the available data. In practice, a variety of strategies are employed to tackle this issue. One common approach is **grid search**, where the hyperparameter space is discretized, and a set of combinations is tested exhaustively. Another approach is **random search**, where the hyperparameters are sampled randomly from a given distribution. **Bayesian optimization** is another popular method that models the performance of the learning algorithm as a function of hyperparameters and updates the distribution over the hyperparameter space to minimize the expected loss.\n",
        "\n",
        "In this practical, we will explore the grid search method of hyperparameter tuning. By the end of this practical, you will have a better understanding of how to systematically search for the hyperparameters of your learning algorithm to achieve optimal performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "df946ee4",
      "metadata": {
        "id": "df946ee4"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import check_random_state\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "bef756d2",
      "metadata": {
        "id": "bef756d2"
      },
      "source": [
        "### 1. Load the dataset\n",
        "A subset of the MNIST database of handwritten digits is used in this example. The dataset consists of 70,000 image samples belonging to 10 digit classes. The digits in the dataset have been standardized in terms of size and centered within fixed-size images. The original black and white images from NIST were adjusted to fit a 20x20 pixel box while maintaining their aspect ratio. As a result of the normalization algorithm's anti-aliasing technique, the images now contain varying shades of gray. Additionally, the images were further centered within a 28x28 image by calculating the center of mass of the pixels and shifting the image to position this point at the center of the 28x28 field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db586a7b",
      "metadata": {
        "id": "db586a7b"
      },
      "outputs": [],
      "source": [
        "# Load data from https://www.openml.org/d/554\n",
        "X, y = fetch_openml(\"mnist_784\", version=1,  return_X_y=True, as_frame=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e22174d3",
      "metadata": {
        "id": "e22174d3"
      },
      "source": [
        "#### 2. **Perform dataset inspection:**\n",
        "Perform taks like previweing image samples from each class in the dataset, exploring the dataset dimensions, examining the dataset's structure, and examining and visualizing the class distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beb1d33d",
      "metadata": {
        "id": "beb1d33d"
      },
      "outputs": [],
      "source": [
        "# Generate train test splits (Considering the constraints of limited time and resources available for experimentation, use only 5000 samaples for training and 1000 samples for testing)\n",
        "train_samples = 5000\n",
        "test_samples = 1000\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_samples, test_size=test_samples, random_state = 400, stratify = y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d0aea579",
      "metadata": {
        "id": "d0aea579"
      },
      "outputs": [],
      "source": [
        "#Preprocess or standardize the train and test sets using the correct approach: use StandardScaler"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "fK76uI-E3Nu3",
      "metadata": {
        "id": "fK76uI-E3Nu3"
      },
      "source": [
        "\n",
        "#### 3. **Train a multiclass logistic regression classifier**\n",
        "\n",
        "Multiclass classification is a type of supervised learning problem where the goal is to assign an input sample to one of several classes. It involves classifying data into more than two mutually exclusive classes. \n",
        "\n",
        "Multinomial logistic regression is a well-known method for multiclass classification which provides a probabilistic framework for assigning class labels. It can handle cases where the classes are not linearly separable and allows for more complex decision boundaries in the feature space. This makes it suitable for a wide range of applications, including image recognition, text categorization, and multi-class sentiment analysis.\n",
        "\n",
        "**Train a multinomial logistic regression model on the preprocessed data (scikit learn LogisticRegression())**: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "\n",
        "\n",
        "The following are some important hyperparameters that can significantly influence the performance of the model:\n",
        "\n",
        "**regularization type (penalty = {'l1', 'l2', 'elasticnet', None}, default='l2')** \n",
        "\n",
        "**regularization strength (C)**\n",
        "\n",
        "**class_weight**\n",
        "\n",
        "**solver algorithm (solver)** \n",
        "\n",
        "**and maximum iterations (max_iter)** \n",
        "\n",
        "You task is to systematically tune these hyperparameters by conducting experiments and evaluating the model's performance with different parameter combinations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "b10c7c1c",
      "metadata": {
        "id": "b10c7c1c"
      },
      "outputs": [],
      "source": [
        "#Train a multinomial logistic regression model on the preprocessed data using the default parameters.\n",
        "#Scikit-learn page on how to train the model: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "# Call this model the deault_model. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e713cc2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute the accuracy (or error) of the default_model on the training set "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b68998e",
      "metadata": {
        "id": "6b68998e"
      },
      "outputs": [],
      "source": [
        "# Compute the accuracy (or error) on the test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc78be91",
      "metadata": {},
      "source": [
        "Overfitting: When a model becomes too complex and fits the training data exceptionally well, but struggles to generalize to new, unseen test data.\n",
        "\n",
        "Underfitting: Occurs when a model is too simple or lacks complexity, resulting in poor performance on both the training and test data because it fails to capture the underlying patterns in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f9d90397",
      "metadata": {
        "id": "f9d90397"
      },
      "outputs": [],
      "source": [
        "# Are there any issues of overfitting or underfitting?\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "K8BrZtWG6SAg",
      "metadata": {
        "id": "K8BrZtWG6SAg"
      },
      "source": [
        "#### 4. Train a different model with different set of hyperparameters: \n",
        "For example,  use C = 50.0/train_samples, penalty=\"l1\", solver=\"saga\", tol=0.1\n",
        "\n",
        "Compute the train and test set accuracy. Did the train and test set accuracy improve?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "878d0afd",
      "metadata": {
        "id": "878d0afd"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "wj17EiFT62q2",
      "metadata": {
        "id": "wj17EiFT62q2"
      },
      "source": [
        "#### 5. Question\n",
        "Is it considered a valid and appropriate practice to tune the model parameters solely based on evaluating the performance on the train and test sets? Are there any limitations or potential drawbacks to this approach?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10a1b875",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "19289b71",
      "metadata": {
        "id": "19289b71"
      },
      "source": [
        "#### 6. Grid Search\n",
        "\n",
        "As you notice in the above tasks, manual hyperparameter tuning requires lots of effort and experiments. what alternative approaches can you suggest to mitigate the significant time and effort involved in the process?\n",
        "\n",
        "\n",
        "Utilizing **grid search within a cross-validation framework** is a more systematic and accurate method of tuning an algorithm's hyperparameters. Instead of dividing the dataset into solely a train and test set, it is divided into three sets: a train set, a test set, and a validation set. The parameters are then tuned based on the validation set's performance in the cross-validation framework. The test set is reserved solely for reporting the model's accuracy on unseen data.\n",
        "\n",
        "Scikit-learn provides functions for performing grid search: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV\n",
        "\n",
        "**Perform the following task**: \n",
        "\n",
        "Perform grid search on the parameters to obtain your best performing model. Use a 10-fold cross-validation strategy. Use 'accuracy' as the scoring metric (maximize the accuracy). Evaluate and tune the parameters: C, penalty, tol, and solver to achieve the best results.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "460fb258",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Implement grid search here. See https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8bfd0616",
      "metadata": {
        "id": "8bfd0616"
      },
      "outputs": [],
      "source": [
        "#What set of a hyperparameters did the grid searh found in your experiment? \n",
        "\n",
        "\n",
        "#What is the achieved accuracy? Is is better or worse than the manually set hyperparameters in the previous taks. \n",
        "\n",
        "\n",
        "#Analyze the results to gain a better understanding of the relationship between hyperparameters and model performance.\n",
        "\n",
        "\n",
        "\n",
        "#Is accuracy alone a good metric for evaluating the performance?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "Xcmez_Iz-80T",
      "metadata": {
        "id": "Xcmez_Iz-80T"
      },
      "source": [
        "#### 7. Questions\n",
        "Grid search is known to be computationally extensive. What makes grid search a computationally expensive technique?\n",
        "\n",
        "Can you suggest some approaches to make grid search more efficient in terms of computational resources and time?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "340e9fe7",
      "metadata": {
        "id": "340e9fe7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
